{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.   Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boto3, pandas and numpy packages\n",
    "import boto3  \n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket, match=''):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    for key in bucket_resource.objects.all():\n",
    "        if match in key.key:\n",
    "            print(key.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function you defined to list the file of the 'nyc-tlc' bucket that contains the string '2020'\n",
    "# Load the file named trip data/yellow_tripdata_2020-04.csv into a dataframe called df. Specify s3:// as prefix for the file url\n",
    "\n",
    "# variable `bucket_name` that will contain the name of the bucket `nyc-tlc` dataset\n",
    "bucket_name = 'nyc-tlc'\n",
    "\n",
    "#------------------------------------------------------\n",
    "starttime = timeit.default_timer()\n",
    "list_bucket_contents(bucket=bucket_name, match='yellow_tripdata_2020-04')\n",
    "print('\\nTime taken is {:0.2f}'.format(timeit.default_timer() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimensions (shape) of df\n",
    "\n",
    "# Display the summary (info) of df\n",
    "\n",
    "# Display the descriptive statistics of df\n",
    "\n",
    "# Save the dataframe locally in the data/raw folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "def write_nyc_data(df_data, data_set='', parquet_name='', partition_by_list=''):\n",
    "    # write data to parquet\n",
    "    starttime = timeit.default_timer()\n",
    "    df_data.write.partitionBy(partition_by_list).parquet(parquet_name, mode='append') \n",
    "    print('Write Parquet ({}) Total Time taken : {:0.2f}'.format(data_set, timeit.default_timer() - starttime), 'seconds')\n",
    "    \n",
    "    # write 1 monthy file\n",
    "    df.repartition(1).write.csv(f'{nyc_csv}/{data_set}/{year_month}', header=\"true\", mode=\"append\") \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "\n",
    "Create a copy of df and save it into a variable called df_cleaned\n",
    "\n",
    "Launch magic commands to automatically reload modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df and save it into a variable called df_cleaned\n",
    "df_cleaned = df_data.data()\n",
    "\n",
    "# Launch magic commands to automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.4]** Import your new function `convert_to_date` from `src.features.dates`\n",
    "\n",
    "**[3.5]** Convert the column `tpep_pickup_datetime`, `tpep_dropoff_datetime` with your function `convert_to_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your new function convert_to_date from src.features.dates\n",
    "from src.features.dates import convert_to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.7]** Convert the `trip_duration` column into 5 different bins with [0, 300, 600, 1800, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['trip_duration'] = pd.cut(df_cleaned['trip_duration'], bins=[0, 300, 600, 1800, 100000], labels=['x<5min', 'x<10min', 'x<30min', 'x>=30min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.8]** Extract the month component from `tpep_pickup_datetime` and save the results in the column `tpep_pickup_dayofmonth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month name component from dteday and save the results in the column mnth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.10]** Extract the day of week component from `tpep_pickup_datetime` and save the results in the column `tpep_pickup_dayofweek`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract the day of week component from dteday and save the results in the column weekday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.11]** Perform One-Hot encoding on the categorical features (`VendorID`, `RatecodeID`, `store_and_fwd_flag`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot encoding on the categorical features (VendorID, RatecodeID, store_and_fwd_flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.12]** Drop the columns `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`\n",
    "\n",
    "**[3.13]** Save the prepared dataframe in the `data/interim` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`\n",
    "\n",
    "\n",
    "# Save the prepared dataframe in the `data/interim` folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split the Dataset\n",
    "\n",
    "**[4.1]** In the file `src/data/sets.py` create a function called `pop_target` with the following logics:\n",
    "- input parameters: dataframe (`df`), target column name (`target_col`), flag to convert to Numpy array which False by default (`to_numpy`)\n",
    "- logics: extract the target variable from input dataframe, split the input dataframe into training, validation and testing sets from the specified ratio\n",
    "- output parameters: features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called pop_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.3]** Import your new function `split_sets_random` and split the data into several sets as Numpy arrays\n",
    "\n",
    "**[4.4]** Import save_sets from src.data.sets and save the sets into the folder `data/processed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import save_sets from src.data.sets and save the sets into the folder data/processed\n",
    "from src.data.sets import split_sets_random, save_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model\n",
    "\n",
    "**[5.1]** in `src.models` folder, create a script called `null.py` ans define a class called "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NullModel' from 'src.models.null' (/home/jovyan/work/src/models/null.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6071b7fab9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import NullModel from src.models.null\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNullModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NullModel' from 'src.models.null' (/home/jovyan/work/src/models/null.py)"
     ]
    }
   ],
   "source": [
    "# Import NullModel from src.models.null\n",
    "from src.models.null import NullModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.3]** Instantiate a `NullModel` with `target_type='classification'` and save it into a variable called `base_model`\n",
    "\n",
    "**[5.4]** Make a prediction using `fit_predict()` and save the results in a variable called `y_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using fit_predict() and save the results in a variable called y_base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.5]** In the `src/models/performance.py` file, create a function called `print_class_perf` with the following logics:\n",
    "- input parameters: predicted target (`y_preds`), actual target (`y_actuals`) and name of the set (`set_name`)\n",
    "- logics: Print the Accuracy and F1 score for the provided data\n",
    "- output parameters: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the src/models/performance.py file, create a function called print_class_perf \n",
    "\n",
    "# Display the Accuracy and F1 scores of this baseline model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 6. Push changes\n",
    "\n",
    "# Add you changes to git staging area\n",
    "# Create the snapshot of your repository and add a description\n",
    "# Push your snapshot to Github\n",
    "\n",
    "git add .   \n",
    "git commit -m \"commit 2 S3 resource 'nyc-tlc'\"\n",
    "git push https://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@github.com/CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    "# Check out to the master branch\n",
    "# Pull the latest updates\n",
    "\n",
    "git checkout master\n",
    "git pull https://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@github.com/CazMayhem/adv_dsi_lab_2.git\n",
    "\n",
    "# Check out to the data_prep branch\n",
    "# Merge the master branch and push your changes\n",
    "git checkout data_prep\n",
    "git merge master\n",
    "git push https://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@github.com/CazMayhem/adv_dsi_lab_2.git\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.8]** Go to Github and merge the branch after reviewing the code and fixing any conflict\n",
    "\n",
    "**[6.9]** Stop the Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Docker container\n",
    "# docker stop adv_dsi_lab_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
