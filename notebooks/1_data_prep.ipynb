{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Start Docker Containeer\n",
    "docker run  -dit --rm --name adv_dsi_lab_3 -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v ~/Projects/adv_dsi/adv_dsi_lab_3:/home/jovyan/work -v ~/.aws:/home/jovyan/.aws -v ~/Projects/adv_dsi/src:/home/jovyan/work/src xgboost-notebook:latest \n",
    "docker logs --tail 50 adv_dsi_lab_3\n",
    "\n",
    "# Initialise the repo\n",
    "git init\n",
    "\n",
    "# In your local repo adv_dsi_lab_1, link it with Github \n",
    "git remote add origin git@github.com:CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    " # if you make a mistake\n",
    " # git remote remove origin \n",
    "\n",
    "# Push Content - Add you changes to git staging area\n",
    "# Create the snapshot of your repository and add a description\n",
    "# Push your snapshot to Github\n",
    "\n",
    "git add .\n",
    "git commit -m \"first commit lab 3\"\n",
    "git push https://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@github.com/CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    "#  Preventing push to master branch\n",
    "\n",
    "git config branch.master.pushRemote no_push\n",
    "\n",
    "# Check out to the master branch, Pull the latest updates\n",
    "\n",
    "git checkout master\n",
    "git pull https://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@github.com/CazMayhem/adv_dsi_lab_2.git\n",
    "\n",
    "# Create a new git branch called data_prep\n",
    "\n",
    "git checkout -b data_prep\n",
    "\n",
    "# Navigate the folder notebooks and create a new jupyter notebook called 1_data_prep.ipynb\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.   Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boto3, pandas and numpy packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3  \n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['AWS_PROFILE'] = \"S3_dev\"\n",
    "\n",
    "def list_bucket_contents(bucket, match=''):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    for key in bucket_resource.objects.all():\n",
    "        if match in key.key:\n",
    "            print(key.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip data/fhv_tripdata_2020-01.csv\n",
      "trip data/fhv_tripdata_2020-02.csv\n",
      "trip data/fhv_tripdata_2020-03.csv\n",
      "trip data/fhv_tripdata_2020-04.csv\n",
      "trip data/fhv_tripdata_2020-05.csv\n",
      "trip data/fhv_tripdata_2020-06.csv\n",
      "trip data/fhv_tripdata_2020-07.csv\n",
      "trip data/fhv_tripdata_2020-08.csv\n",
      "trip data/fhv_tripdata_2020-09.csv\n",
      "trip data/fhv_tripdata_2020-10.csv\n",
      "trip data/fhv_tripdata_2020-11.csv\n",
      "trip data/fhv_tripdata_2020-12.csv\n",
      "trip data/fhvhv_tripdata_2020-01.csv\n",
      "trip data/fhvhv_tripdata_2020-02.csv\n",
      "trip data/fhvhv_tripdata_2020-03.csv\n",
      "trip data/fhvhv_tripdata_2020-04.csv\n",
      "trip data/fhvhv_tripdata_2020-05.csv\n",
      "trip data/fhvhv_tripdata_2020-06.csv\n",
      "trip data/fhvhv_tripdata_2020-07.csv\n",
      "trip data/fhvhv_tripdata_2020-08.csv\n",
      "trip data/fhvhv_tripdata_2020-09.csv\n",
      "trip data/fhvhv_tripdata_2020-10.csv\n",
      "trip data/fhvhv_tripdata_2020-11.csv\n",
      "trip data/fhvhv_tripdata_2020-12.csv\n",
      "trip data/green_tripdata_2020-01.csv\n",
      "trip data/green_tripdata_2020-02.csv\n",
      "trip data/green_tripdata_2020-03.csv\n",
      "trip data/green_tripdata_2020-04.csv\n",
      "trip data/green_tripdata_2020-05.csv\n",
      "trip data/green_tripdata_2020-06.csv\n",
      "trip data/green_tripdata_2020-07.csv\n",
      "trip data/green_tripdata_2020-08.csv\n",
      "trip data/green_tripdata_2020-09.csv\n",
      "trip data/green_tripdata_2020-10.csv\n",
      "trip data/green_tripdata_2020-11.csv\n",
      "trip data/green_tripdata_2020-12.csv\n",
      "trip data/yellow_tripdata_2020-01.csv\n",
      "trip data/yellow_tripdata_2020-02.csv\n",
      "trip data/yellow_tripdata_2020-03.csv\n",
      "trip data/yellow_tripdata_2020-04.csv\n",
      "trip data/yellow_tripdata_2020-05.csv\n",
      "trip data/yellow_tripdata_2020-06.csv\n",
      "trip data/yellow_tripdata_2020-07.csv\n",
      "trip data/yellow_tripdata_2020-08.csv\n",
      "trip data/yellow_tripdata_2020-09.csv\n",
      "trip data/yellow_tripdata_2020-10.csv\n",
      "trip data/yellow_tripdata_2020-11.csv\n",
      "trip data/yellow_tripdata_2020-12.csv\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'nyc-tlc'\n",
    "\n",
    "list_bucket_contents(bucket_name,'2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken is 30.54\n"
     ]
    }
   ],
   "source": [
    "# Call the function you defined to list the file of the 'nyc-tlc' bucket that contains the string '2020'\n",
    "# Load the file named trip data/yellow_tripdata_2020-04.csv into a dataframe called df. \n",
    "# Specify s3:// as prefix for the file url\n",
    "\n",
    "# variable `bucket_name` that will contain the name of the bucket `nyc-tlc` dataset\n",
    "bucket_name = 'nyc-tlc'\n",
    "\n",
    "#------------------------------------------------------\n",
    "starttime = timeit.default_timer()\n",
    "df = pd.read_csv(f's3a://{bucket_name}/trip data/yellow_tripdata_2020-04.csv')\n",
    "print('\\nTime taken is {:0.2f}'.format(timeit.default_timer() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237993, 18)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 237993 entries, 0 to 237992\n",
      "Data columns (total 18 columns):\n",
      "VendorID                 218480 non-null float64\n",
      "tpep_pickup_datetime     237993 non-null object\n",
      "tpep_dropoff_datetime    237993 non-null object\n",
      "passenger_count          218480 non-null float64\n",
      "trip_distance            237993 non-null float64\n",
      "RatecodeID               218480 non-null float64\n",
      "store_and_fwd_flag       218480 non-null object\n",
      "PULocationID             237993 non-null int64\n",
      "DOLocationID             237993 non-null int64\n",
      "payment_type             218480 non-null float64\n",
      "fare_amount              237993 non-null float64\n",
      "extra                    237993 non-null float64\n",
      "mta_tax                  237993 non-null float64\n",
      "tip_amount               237993 non-null float64\n",
      "tolls_amount             237993 non-null float64\n",
      "improvement_surcharge    237993 non-null float64\n",
      "total_amount             237993 non-null float64\n",
      "congestion_surcharge     237993 non-null float64\n",
      "dtypes: float64(13), int64(2), object(3)\n",
      "memory usage: 32.7+ MB\n",
      "None\n",
      "<bound method NDFrame.describe of         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0            1.0  2020-04-01 00:41:22   2020-04-01 01:01:53              1.0   \n",
      "1            1.0  2020-04-01 00:56:00   2020-04-01 01:09:25              1.0   \n",
      "2            1.0  2020-04-01 00:00:26   2020-04-01 00:09:25              1.0   \n",
      "3            1.0  2020-04-01 00:24:38   2020-04-01 00:34:38              0.0   \n",
      "4            2.0  2020-04-01 00:13:24   2020-04-01 00:18:26              1.0   \n",
      "...          ...                  ...                   ...              ...   \n",
      "237988       NaN  2020-04-30 19:47:00   2020-04-30 20:07:00              NaN   \n",
      "237989       NaN  2020-04-30 19:34:00   2020-04-30 19:39:00              NaN   \n",
      "237990       NaN  2020-04-30 19:58:14   2020-04-30 20:03:47              NaN   \n",
      "237991       NaN  2020-04-30 19:35:15   2020-04-30 19:56:51              NaN   \n",
      "237992       NaN  2020-04-30 19:27:47   2020-04-30 19:45:00              NaN   \n",
      "\n",
      "        trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
      "0                1.20         1.0                  N            41   \n",
      "1                3.40         1.0                  N            95   \n",
      "2                2.80         1.0                  N           237   \n",
      "3                2.60         1.0                  N            68   \n",
      "4                1.44         1.0                  Y           263   \n",
      "...               ...         ...                ...           ...   \n",
      "237988          10.41         NaN                NaN            42   \n",
      "237989           0.93         NaN                NaN            42   \n",
      "237990           2.44         NaN                NaN           137   \n",
      "237991           6.68         NaN                NaN           137   \n",
      "237992           4.35         NaN                NaN            36   \n",
      "\n",
      "        DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
      "0                 24           2.0         5.50    0.5      0.5        0.00   \n",
      "1                197           1.0        12.50    0.5      0.5        2.75   \n",
      "2                137           1.0        10.00    3.0      0.5        1.00   \n",
      "3                142           1.0        10.00    3.0      0.5        1.00   \n",
      "4                 74           1.0         6.50    0.5      0.5        3.00   \n",
      "...              ...           ...          ...    ...      ...         ...   \n",
      "237988             3           NaN        28.82    0.0      0.5        0.00   \n",
      "237989            74           NaN         5.05    0.0      0.5        0.00   \n",
      "237990             4           NaN         8.09    0.0      0.0        0.00   \n",
      "237991           198           NaN        22.42    0.0      0.5        0.00   \n",
      "237992            65           NaN        16.41    0.0      0.5        0.00   \n",
      "\n",
      "        tolls_amount  improvement_surcharge  total_amount  \\\n",
      "0               0.00                    0.3          6.80   \n",
      "1               0.00                    0.3         16.55   \n",
      "2               0.00                    0.3         14.80   \n",
      "3               0.00                    0.3         14.80   \n",
      "4               0.00                    0.3         13.30   \n",
      "...              ...                    ...           ...   \n",
      "237988          0.00                    0.3         29.62   \n",
      "237989          0.00                    0.3          5.85   \n",
      "237990          0.00                    0.3         10.89   \n",
      "237991          6.12                    0.3         31.84   \n",
      "237992          0.00                    0.3         17.21   \n",
      "\n",
      "        congestion_surcharge  \n",
      "0                        0.0  \n",
      "1                        0.0  \n",
      "2                        2.5  \n",
      "3                        2.5  \n",
      "4                        2.5  \n",
      "...                      ...  \n",
      "237988                   0.0  \n",
      "237989                   0.0  \n",
      "237990                   2.5  \n",
      "237991                   2.5  \n",
      "237992                   0.0  \n",
      "\n",
      "[237993 rows x 18 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions (shape) of df\n",
    "print(df.shape)\n",
    "\n",
    "# Display the summary (info) of df\n",
    "print(df.info())\n",
    "\n",
    "# Display the descriptive statistics of df\n",
    "print(df.describe)\n",
    "\n",
    "df.to_csv('../data/raw/yellow_tripdata_2020-04.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "\n",
    "Create a copy of df and save it into a variable called df_cleaned\n",
    "\n",
    "Launch magic commands to automatically reload modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of df and save it into a variable called df_cleaned\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Launch magic commands to automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.4]** Import your new function `convert_to_date` from `src.features.dates`\n",
    "\n",
    "**[3.5]** Convert the column `tpep_pickup_datetime`, `tpep_dropoff_datetime` with your function `convert_to_date`\n",
    "\n",
    "**[3.6]** Create a new column `trip_duration` that will corresponds to the diuration of the trip in seconds (difference between `tpep_pickup_datetime` and `tpep_dropoff_datetime`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your new function convert_to_date from src.features.dates\n",
    "from src.features.dates import convert_to_date\n",
    "\n",
    "# Convert the column tpep_pickup_datetime, tpep_dropoff_datetime with your function convert_to_date\n",
    "df_cleaned = convert_to_date(df_cleaned, ['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "# Create a new column trip_duration that will corresponds to the diuration of the trip in seconds (difference between tpep_pickup_datetime and tpep_dropoff_datetime)\n",
    "df_cleaned['trip_duration'] = (df_cleaned['tpep_dropoff_datetime'] - df_cleaned['tpep_pickup_datetime']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins - to covert regresssion into classification problem\n",
    "\n",
    "**[3.7]** Convert the `trip_duration` column into 5 different bins with [0, 300, 600, 1800, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['trip_duration'] = pd.cut(df_cleaned['trip_duration'], bins=[0, 300, 600, 1800, 100000], labels=['x<5min', 'x<10min', 'x<30min', 'x>=30min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.8]** Extract the month component from `tpep_pickup_datetime` and save the results in the column `tpep_pickup_dayofmonth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['tpep_pickup_dayofmonth'] = df_cleaned['tpep_pickup_datetime'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.9]** Extract the hour component from `tpep_pickup_datetime` and save the results in the column `tpep_pickup_hourofday`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month name component from dteday and save the results in the column mnth\n",
    "df_cleaned['tpep_pickup_hourofday'] = df_cleaned['tpep_pickup_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.10]** Extract the day of week component from `tpep_pickup_datetime` and save the results in the column `tpep_pickup_dayofweek`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract the day of week component from dteday and save the results in the column weekday\n",
    "df_cleaned['tpep_pickup_dayofweek'] = df_cleaned['tpep_pickup_datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.11]** Perform One-Hot encoding on the categorical features (`VendorID`, `RatecodeID`, `store_and_fwd_flag`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot encoding on the categorical features (VendorID, RatecodeID, store_and_fwd_flag)\n",
    "df_cleaned = pd.get_dummies(df_cleaned, columns=['VendorID', 'RatecodeID', 'store_and_fwd_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.11a]** Check for Null values in the target rows and remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null target values   : 289\n",
      "before drop target na: 237993\n",
      "after drop target na : 237704\n",
      "rows dropped         : 289\n"
     ]
    }
   ],
   "source": [
    "# view target values - are there any nulls or other wierd stuff\n",
    "pd.DataFrame(df_cleaned['trip_duration']).sort_values(by=['trip_duration']).to_csv('../data/target.csv')\n",
    "\n",
    "# are there any NULL values in the Target rows\n",
    "print('Null target values   :',df_cleaned['trip_duration'].isnull().values.sum() )\n",
    "\n",
    "# drop the null target values as it causes the predictions / scoring to fail\n",
    "pre_delete = len(df_cleaned)\n",
    "print('before drop target na:',len(df_cleaned))\n",
    "df_cleaned = df_cleaned.dropna(subset=['trip_duration'])\n",
    "print('after drop target na :',len(df_cleaned))\n",
    "print('rows dropped         :',pre_delete-len(df_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.12]** Drop the columns `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`\n",
    "\n",
    "**[3.13]** Save the prepared dataframe in the `data/interim` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`\n",
    "df_cleaned.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID'], axis=1, inplace=True)\n",
    "\n",
    "# Save the prepared dataframe in the `data/interim` folder\n",
    "df_cleaned.to_csv('../data/interim/yellow_tripdata_2020-04_prepared.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split the Dataset\n",
    "\n",
    "**[4.1]** In the file `src/data/sets.py` create a function called `pop_target` with the following logics:\n",
    "- input parameters: dataframe (`df`), target column name (`target_col`), flag to convert to Numpy array which False by default (`to_numpy`)\n",
    "- logics: extract the target variable from input dataframe, split the input dataframe into training, validation and testing sets from the specified ratio\n",
    "- output parameters: features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called pop_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.3]** Import your new function `split_sets_random` and split the data into several sets as Numpy arrays\n",
    "\n",
    "**[4.4]** Import save_sets from src.data.sets and save the sets into the folder `data/processed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import save_sets from src.data.sets and save the sets into the folder data/processed\n",
    "from src.data.sets import split_sets_random\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_sets_random(df_cleaned, target_col='trip_duration', test_ratio=0.2, to_numpy=True)\n",
    "\n",
    "# Import save_sets from src.data.sets and save the sets into the folder data/processed\n",
    "from src.data.sets import save_sets\n",
    "\n",
    "save_sets(X_train, y_train, X_val, y_val, X_test, y_test, path='../data/processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model\n",
    "\n",
    "**[5.1]** in `src.models` folder, create a script called `null.py` ans define a class called "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NullModel from src.models.null\n",
    "from src.models.null import NullModel\n",
    "\n",
    "# Instantiate a NullModel with target_type='classification and save it into a variable called base_model\n",
    "base_model = NullModel(target_type=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.3]** Instantiate a `NullModel` with `target_type='classification'` and save it into a variable called `base_model`\n",
    "\n",
    "**[5.4]** Make a prediction using `fit_predict()` and save the results in a variable called `y_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using fit_predict() and save the results in a variable called y_base\n",
    "\n",
    "y_base = base_model.fit_predict(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_train).sort_values(by=[0]).to_csv('../data/y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.5]** In the `src/models/performance.py` file, create a function called `print_class_perf` with the following logics:\n",
    "- input parameters: predicted target (`y_preds`), actual target (`y_actuals`) and name of the set (`set_name`)\n",
    "- logics: Print the Accuracy and F1 score for the provided data\n",
    "- output parameters: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.34593540968434044\n",
      "F1 Training      : 0.17782622674521764\n"
     ]
    }
   ],
   "source": [
    "# In the src/models/performance.py file, create a function called print_class_perf \n",
    "from src.models.performance import print_class_perf\n",
    "\n",
    "# Display the Accuracy and F1 scores of this baseline model on the training set\n",
    "print_class_perf(y_preds=y_base, y_actuals=np.array(y_train), set_name='Training', average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 6. Push changes\n",
    "\n",
    "# Add you changes to git staging area\n",
    "# Create the snapshot of your repository and add a description\n",
    "# Push your snapshot to Github\n",
    "\n",
    "git add .   \n",
    "git commit -m \"commit 2 S3 resource 'nyc-tlc'\"\n",
    "git push https://*********@github.com/CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    "# Check out to the master branch\n",
    "# Pull the latest updates\n",
    "\n",
    "git checkout master\n",
    "git pull https://*********@github.com/CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    "# Check out to the data_prep branch\n",
    "# Merge the master branch and push your changes\n",
    "git checkout data_prep\n",
    "git merge master\n",
    "git push https://*********@github.com/CazMayhem/adv_dsi_lab_3.git\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.8]** Go to Github and merge the branch after reviewing the code and fixing any conflict\n",
    "\n",
    "**[6.9]** Stop the Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Docker container\n",
    "# docker stop adv_dsi_lab_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
